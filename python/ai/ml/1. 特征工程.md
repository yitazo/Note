## 1. 数据集

### 1.1 数据集概念

数据集由特征值和目标值组成，有的数据集没有目标值

根据数据集类别的不同，有以下不同的机器学习算法

监督学习：有目标值

- 目标值是离散的 - 分类问题（人脸识别、识别猫狗、预测明天天气类别）
  - 分类算法有k-近邻算法、贝叶斯分类、决策树与随机森林、逻辑回归
- 目标值是连续的 - 回归问题（预测人脸的具体年龄、预测明天温度）
  - 回归算法有线性回归、岭回归

无监督学习：没有目标值

- 没有目标值 - 聚类问题（将数据分成n堆，物以类聚）
  - 聚类算法有k-means聚类算法

### 1.2 数据集来源

学习阶段可以用的数据集：

- sklearn
- kaggle
- UCI

### 1.3 sklearn数据集

#### 1.3.1 获取数据集API

通过sklearn.datasets这个包里面的方法来获取，里面有两类方法load_xxx()和fetch_xxx()

- sklearn小数据集，无需联网下载，pip安装的时候已经下载在包里

  - sklearn.datasets.load_iris()
  - sklearn.datasets.load_xxx()

- sklearn大数据集，需要联网下载

  - sklearn.datasets.fetch_20newsgroups(data_home=None,subset=‘train’)

    - data_home为下载的数据放在电脑的哪里，默认下载到home目录下

    - subset可选值有train、test、all，分别表示训练集、测试集、全部

  - sklearn.datasets.fetch_xxx()

#### 1.3.2 返回值

上面两类方法的的返回值是datasets.base.Bunch，该类型继承自字典

- 可通过dict["key"] = values或bunch.key = values这两种方式操作里面的内容
- 返回值里有以下属性
  - DESCR：数据集的描述信息
  - feature_names：特征名称，是一维ndarray
  - data：特征值，是二维ndarray
  - target_names：目标名称，是一维ndarray
  - target：目标值，与target_names对应，是一维ndarray

### 1.4 数据集划分

#### 1.4.1 原因

由于最后需要对模型进行评估，因此数据集需要保留一部分用于评估模型

- 训练数据：用于训练、构建模型
- 测试数据：在模型检验时使用，用于评估模型是否有效，测试集一般占20% ~ 30%

#### 1.4.2 数据集划分API

使用sklearn.model_selection.train_test_split(arrays, *options)进行划分，arrays将data和target按顺序传入即可，options有以下可选参数

- test_size：浮点数，代表测试集的比例大小，默认值是0.25
- random_state：整数，代表随机数种子，测试不同算法的优劣时使用相同的随机数种子

返回值为训练集特征值、测试集特征值、训练集目标值、测试集目标值，一般命名为x_train、x_test、y_train、y_test

## 2. 特征工程

数据和特征决定机器学习的上限，而模型和算法只是逼近这个上限

### 2.1 特征工程的位置和内容

1. 获取数据
2. 数据清洗和预处理（使用pandas）
3. 特征工程（使用sklearn）
   - 特征提取/抽取
   - 特征预处理
   - 特征降维
4. 机器学习算法训练模型
5. 模型评估
6. 应用

### 2.2 特征提取/抽取

机器学习算法其实就是统计方法，或者说是数学公式，因此需要进行特征提取，将任意类型数据转化为数值特征让计算机理解

主要有以下类型特征提取：

- 字典（类别）特征提取
- 文本特征提取
- 图像特征提取（深度学习里面介绍）

#### 2.2.1 特征提取API

在sklearn中是使用sklearn.feature_extraction这个包下的方法进行特征提取

#### 2.2.1 字典（类别）特征提取

例如某一特征是类别，使用字符串存储，那我们可以将其转换成one-hot编码或者哑变量

在sklearn中是使用sklearn.feature_extraction.DictVectorizer(sparse=True,…)来进行字典（类别）特征提取

- DictVectorizer.fit_transform(data)  data：字典或者包含字典的迭代器，默认返回值是sparse矩阵，设置False返回二维数组
- DictVectorizer.inverse_transform(data_new)  data_new：二维数组或sparse矩阵，返回转换前的格式
- DictVectorizer.get_feature_names_out()  返回转换之后的feature_names()，一维数组表示特征名称

```python
from sklearn.feature_extraction import DictVectorizer

data = [{"city": "北京", "temperature": 100},
        {"city": "上海", "temperature": 80},
        {"city": "深圳", "temperature": 30}]

# 1. 实例化转换器类，默认sparse=True返回值结果是稀疏矩阵，每行使用元组加值表示非零元素
transfer = DictVectorizer(sparse=False)

# 2. 调用fit_transform，传入单个字典或者包含字典的转换器，默认返回值是sparse稀疏矩阵
data_new = transfer.fit_transform(data)

# [[  0.   1.   0. 100.]
#  [  1.   0.   0.  80.]
#  [  0.   0.   1.  30.]]
print(data_new)

# ['city=上海' 'city=北京' 'city=深圳' 'temperature']，老版本使用get_feature_names()
print(transfer.get_feature_names_out())
```

#### 2.2.2 文本特征提取

以单词作为特征比较合适，以次数或词频来做值

在sklearn中是使用sklearn.feature_extraction.text.CountVectorizer(stop_words=[])来进行文本特征提取（统计每个样本中特征词个数）

- CountVectorizer.fit_transform(data)  data：文本或包含文本字符的迭代器，返回值是sparse矩阵
- CountVectorizer.inverse_transform(data_new)  data_new：二维数组或sparse矩阵，返回转换前的格式
- CountVectorizer.get_feature_names_out()  返回转换之后的feature_names()，一维数组表示特征名称

注意：

- 中文文本需要进行分词，否则会将整个样本当成特征词

- TfidfVectorizer统计 tf * idf 值，越大代表词越重要，重要的词一般是在当前文章中出现较多，其他文章中出现较少的。用法和CountVectorizer一样

### 2.3 特征预处理

通过一系列函数，将特征数据转换成更加适合算法模型的特征数据过程

包含内容：

- 数值型数据的无量纲化
  - 归一化
  - 标准化

做归一化和标准化的目的是为了让特征同等重要，将不同规格数据转换成同一规格。否则一些特征对结果影响特别小，会导致算法学习不到其他特征规律

#### 2.3.1 特征预处理API

在sklearn中使用的是sklearn.preprocessing这个包进行特征与处理

#### 2.3.2 归一化

将原始数值转化成[mi, mx]的范围，一般使用[0, 1]

公式：
$$
x' = (x - min) / (max - min)
$$

$$
x'' = x' * (mx - mi) + mi
$$

x是当前原始值，min和max是该列最小值和最大值，x''是最终替换x的结果



在sklearn中使用sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1))来进行归一化

- MinMaxScaler.fit_transform(data) data: 二维的ndarray或dataframe，数值类型
  - 返回值是和data形状相同的ndarray

#### 2.3.3 标准化

如果最大值或最小值是异常值的话，归一化的结果会很不理想。归一化鲁棒性较差，只适合传统精确小数据

这种时候要使用标准化，异常值对标准化的平均值和标准差影响很小，标准化比较稳定

公式：
$$
x' = (x - mean) / \sigma
$$
x是当前原始值，mean为该列的平均值，sigma为标准差，x'是最终替换x的结果



在sklearn中使用sklearn.preprocessing.StandardScaler()来进行标准化

- StandardScaler.fit_transform(data) data: 二维的ndarray或dataframe，数值类型
  - 返回值是和data形状相同的ndarray，每一列的数据变成均值为0，标准差为1

### 2.4 特征降维

处理对象是二维数组，降维是指在限定条件下降低随机变量（特征）的个数，得到一组”不相关“的主变量（特征）

通俗讲就是删除相关特征，让特征之间不相关

相关特征（correlated feature）：

- 相对湿度和降雨量之间的关系
- 等等



特征降维大体上分为：

- 特征选择
- 主成分分析（可以理解为一种特征提取的方式）

#### 2.4.1 特征选择

数据中包含冗余或相关特征，从原有特征中找出主要特征。要用数学公式自动找出主要特征

**1 方法**

FIlter（过滤式）：主要探究特征本身特点、特征之间、特征与目标值之间的关联

- 方差选择法：过滤掉低方差的特征，比如鸟分类，某一个特征是是否有爪子，都有爪子方差为0
- 相关系数：该系数衡量特征之间的相关性，过滤掉相关性比较强的特征

Embedded（嵌入式）：算法自动选择特征（特征与目标值之间的关联）

- 决策树：信息熵、信息增益
- 正则化：L1、L2
- 深度学习：卷积等

**2 特征选择API**

在sklearn中使用的是sklearn.feature_selection这个包进行特征与处理

**3 过滤式**

**3.1 低方差特征过滤**

删除掉低方差的特征，方差越低代表该特征大多数样本值越相近，这样的特征意义不大



使用sklearn.feature_selection.VarianceThreshold(threshold=0.0)进行低方差特征过滤（方差小于等于阈值的列会被删掉，默认阈值为0）

- VarianceThreshold.fit_transform(data) data：二维ndarray或dataframe，数值类型
  - 返回值是删除方差小于等于threshold的特征列后的ndarray

**3.2 高相关系数特征过滤**

皮尔逊相关系数可用来反映变量之间相关关系密切程度，相关系数-1 <= r <= 1，r > 0代表正相关，r < 0代表负相关。|r|越接近1代表两特征相关性很强

一般来说，|r| < 0.4代表相关性很弱，0.4 <= |r| < 0.7代表显著相关，0.7 <= |r| <= 1代表高度线性相关



使用scipy.stats.pearsonr(x, y)计算相关系数

- x、y是一维ndarray或者series
- 返回结果有两个数，第一个数是相关系数，表示相关程度，第二个系数是显著水平，小于0.05（0.10）代表是相关的

#### 2.4.2 主成分分析